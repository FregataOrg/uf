import os
import argparse
import base64
import json
import time
from io import BytesIO

from PIL import Image
from langchain_community.chat_models import ChatOllama
from langchain_core.messages import HumanMessage

# --- ã‚°ãƒ­ãƒ¼ãƒãƒ«è¨­å®š ---
INPUT_DIR = "trim_imgs"
OUTPUT_DIR = "outputs"
CACHE_DIR = "analysis_cache"
OLLAMA_MODEL = "z-uo/qwen2.5vl_tools:7b"

# --- ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆå®šç¾© (Qwen-VLå‘ã‘æ”¹å–„ç‰ˆ) ---

# 1. ã‚·ã‚¹ãƒ†ãƒ ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆï¼ˆå½¹å‰²å®šç¾©ï¼‰
# ã“ã‚Œã‚’å„å‘¼ã³å‡ºã—ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®å…ˆé ­ã«è¿½åŠ ã™ã‚‹ã‹ã€SystemMessageã¨ã—ã¦æ¸¡ã—ã¾ã™ã€‚
SYSTEM_PROMPT = """
You are an expert digital archivist specializing in mathematical and scientific texts. Your task is to perform high-fidelity Optical Character Recognition (OCR) and document layout analysis, converting physical pages into perfectly structured Markdown documents with accurate LaTeX formatting.
"""

# 2. ãƒšãƒ¼ã‚¸æ§‹é€ ã‚’è§£æã•ã›ã‚‹ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (ãƒ‘ã‚¹1)
ANALYSIS_PROMPT = f"""{SYSTEM_PROMPT}
Analyze the layout of this page by following these steps:
1.  **Overall Layout**: First, identify the overall layout. Is it single-column, two-column, or something more complex?
2.  **Component Identification**: Second, locate and identify all distinct components: headers, footers, main text body, figures, tables, and most importantly, mathematical formula blocks.
3.  **Challenge Assessment**: Third, note any potential challenges for transcription, such as small font sizes, complex nested formulas, or unusual text flow.

Provide your analysis as a concise JSON object, focusing only on the structural facts.
"""

# 3. é€šå¸¸ã®æŠ½å‡ºãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ (1ãƒ‘ã‚¹å‡¦ç†ç”¨)
# Few-Shotã®ä¾‹ã¯ã“ã“ã«å«ã‚ã‚‹ã®ãŒåŠ¹æœçš„ã§ã™ã€‚
# ã€é‡è¦ã€‘ `PERFECT_EXAMPLE_MARKDOWN` ã¯ã€ã”è‡ªèº«ã§ä½œæˆã—ãŸé«˜å“è³ªãªã‚µãƒ³ãƒ—ãƒ«ã«ç½®ãæ›ãˆã¦ãã ã•ã„ã€‚
PERFECT_EXAMPLE_MARKDOWN = """
...as shown in the equation:

$$ \sum_{i=0}^{n} i = \frac{n(n+1)}{2} $$

This is followed by more text.
"""

BASIC_EXTRACTION_PROMPT = f"""{SYSTEM_PROMPT}
Your task is to transcribe the provided page into a clean Markdown document with perfect LaTeX formatting.

Here is an example of the quality and format required:
--- EXAMPLE START ---
{PERFECT_EXAMPLE_MARKDOWN}
--- EXAMPLE END ---

Now, transcribe the entire page. Your final output MUST be only the Markdown content, starting directly with the first character.
"""

# 4. è§£æçµæœã‚’ä»˜åŠ ã—ã¦é«˜ç²¾åº¦ãªæŠ½å‡ºã‚’è¡Œã†ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã®ãƒ†ãƒ³ãƒ—ãƒ¬ãƒ¼ãƒˆ (ãƒ‘ã‚¹2)
REFINED_EXTRACTION_PROMPT_TEMPLATE = f"""{SYSTEM_PROMPT}
You will perform a highly accurate transcription of the provided page.

First, study this example of the required output quality and format:
--- EXAMPLE START ---
{PERFECT_EXAMPLE_MARKDOWN}
--- EXAMPLE END ---

Next, study the preliminary analysis of this specific page's structure:
--- ANALYSIS ---
{{analysis_text}}
--- END ANALYSIS ---

Considering both the example and the specific analysis, transcribe the entire page into a single, valid Markdown document.
Pay extra close attention to details mentioned in the analysis. Do not include any other commentary.
"""


def image_to_base64(image_path: str) -> str:
    """ç”»åƒã‚’Base64æ–‡å­—åˆ—ã«å¤‰æ›ã™ã‚‹"""
    try:
        with Image.open(image_path) as img:
            buffered = BytesIO()
            if img.mode in ('RGBA', 'P'):
                img = img.convert('RGB')
            img.save(buffered, format="JPEG")
            return base64.b64encode(buffered.getvalue()).decode('utf-8')
    except Exception as e:
        print(f"âŒ ç”»åƒå‡¦ç†ã‚¨ãƒ©ãƒ¼: {image_path}, {e}")
        return None

def call_llm(chat_model, prompt_text, b64_image):
    """LLMã‚’å‘¼ã³å‡ºã—ã€ãƒ¬ã‚¹ãƒãƒ³ã‚¹ã‚’å–å¾—ã™ã‚‹"""
    message = HumanMessage(
        content=[
            {"type": "text", "text": prompt_text},
            {"type": "image_url", "image_url": f"data:image/jpeg;base64,{b64_image}"},
        ]
    )
    try:
        response = chat_model.invoke([message])
        return response.content
    except Exception as e:
        print(f"âŒ Ollamaãƒ¢ãƒ‡ãƒ«å‘¼ã³å‡ºã—ã‚¨ãƒ©ãƒ¼: {e}")
        return None

def analyze_page_structure(chat_model, image_path, cache_path):
    """ãƒšãƒ¼ã‚¸ã®æ§‹é€ ã‚’åˆ†æã™ã‚‹ (ãƒ‘ã‚¹1)"""
    # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒå­˜åœ¨ã™ã‚Œã°åˆ©ç”¨ã™ã‚‹
    if os.path.exists(cache_path):
        print(f"ğŸ§  ã‚­ãƒ£ãƒƒã‚·ãƒ¥åˆ©ç”¨: {os.path.basename(cache_path)}")
        with open(cache_path, 'r', encoding='utf-8') as f:
            return json.load(f).get("analysis_text")

    print(f"ğŸ”¬ ãƒšãƒ¼ã‚¸æ§‹é€ ã‚’è§£æä¸­: {os.path.basename(image_path)}")
    b64_image = image_to_base64(image_path)
    if not b64_image: return None

    analysis_text = call_llm(chat_model, ANALYSIS_PROMPT, b64_image)
    if analysis_text:
        # ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã«ä¿å­˜
        with open(cache_path, 'w', encoding='utf-8') as f:
            json.dump({"analysis_text": analysis_text}, f, ensure_ascii=False, indent=2)
        print(f"ğŸ’¾ è§£æçµæœã‚’ä¿å­˜ã—ã¾ã—ãŸ: {os.path.basename(cache_path)}")
    return analysis_text

def run_basic_process(chat_model, files_to_process):
    """æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ã«åŸºæœ¬çš„ãª1ãƒ‘ã‚¹å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹"""
    print("\n--- ğŸš€ åŸºæœ¬å‡¦ç†ãƒ¢ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ ---")
    for filename in files_to_process:
        input_path = os.path.join(INPUT_DIR, filename)
        output_filename = os.path.splitext(filename)[0] + ".md"
        output_path = os.path.join(OUTPUT_DIR, output_filename)

        if os.path.exists(output_path):
            print(f"â© ã‚¹ã‚­ãƒƒãƒ—: {output_filename} ã¯æ—¢ã«å­˜åœ¨ã—ã¾ã™ã€‚")
            continue

        print(f"ğŸ“„ åŸºæœ¬å‡¦ç†ä¸­: {filename}")
        b64_image = image_to_base64(input_path)
        if not b64_image: continue

        start_time = time.time()
        markdown_content = call_llm(chat_model, BASIC_EXTRACTION_PROMPT, b64_image)
        if markdown_content:
            with open(output_path, "w", encoding="utf-8") as f:
                f.write(markdown_content)
            processing_time = time.time() - start_time
            print(f"âœ… æˆåŠŸ: {output_filename} ã‚’ä½œæˆã—ã¾ã—ãŸã€‚({processing_time:.2f}ç§’)")

def run_refine_process(chat_model, refine_files):
    """æŒ‡å®šã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ç¾¤ã«é«˜ç²¾åº¦ãª2ãƒ‘ã‚¹å‡¦ç†ã‚’å®Ÿè¡Œã™ã‚‹"""
    print("\n--- âœ¨ å†å‡¦ç†ï¼ˆãƒªãƒ•ã‚¡ã‚¤ãƒ³ï¼‰ãƒ¢ãƒ¼ãƒ‰ã‚’é–‹å§‹ã—ã¾ã™ ---")
    for filename in refine_files:
        print(f"\n--- ã‚¿ãƒ¼ã‚²ãƒƒãƒˆ: {filename} ---")
        input_path = os.path.join(INPUT_DIR, filename)
        output_filename = os.path.splitext(filename)[0] + ".md"
        output_path = os.path.join(OUTPUT_DIR, output_filename)
        cache_filename = os.path.splitext(filename)[0] + ".json"
        cache_path = os.path.join(CACHE_DIR, cache_filename)

        # ãƒ‘ã‚¹1: ãƒšãƒ¼ã‚¸æ§‹é€ ã®è§£æ
        analysis_text = analyze_page_structure(chat_model, input_path, cache_path)
        if not analysis_text:
            print(f"âŒ è§£æå¤±æ•—: {filename}")
            continue

        # ãƒ‘ã‚¹2: è§£æçµæœã‚’åˆ©ç”¨ã—ãŸé«˜ç²¾åº¦æŠ½å‡º
        print(f"âœï¸ é«˜ç²¾åº¦æŠ½å‡ºã‚’å®Ÿè¡Œä¸­: {filename}")
        b64_image = image_to_base64(input_path)
        if not b64_image: continue

        start_time = time.time()
        final_prompt = REFINED_EXTRACTION_PROMPT_TEMPLATE.format(analysis_text=analysis_text)
        markdown_content = call_llm(chat_model, final_prompt, b64_image)
        if markdown_content:
            with open(output_path, "w", encoding="utf-8") as f:
                f.write(markdown_content)
            processing_time = time.time() - start_time
            print(f"âœ…âœ… æˆåŠŸ (å†å‡¦ç†): {output_filename} ã‚’æ›´æ–°ã—ã¾ã—ãŸã€‚({processing_time:.2f}ç§’)")

def main():
    """ãƒ¡ã‚¤ãƒ³å‡¦ç†"""
    # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªä½œæˆ
    for dir_path in [OUTPUT_DIR, CACHE_DIR]:
        if not os.path.exists(dir_path):
            os.makedirs(dir_path)

    # ã‚³ãƒãƒ³ãƒ‰ãƒ©ã‚¤ãƒ³å¼•æ•°ã®è¨­å®š
    parser = argparse.ArgumentParser(description="HoTT Bookã®ãƒšãƒ¼ã‚¸ã‚’OCRå‡¦ç†ã—ã€Markdownã«å¤‰æ›ã—ã¾ã™ã€‚")
    parser.add_argument(
        '--refine',
        nargs='+',
        metavar='FILENAME',
        help="æŒ‡å®šã—ãŸãƒ•ã‚¡ã‚¤ãƒ«åã«å¯¾ã—ã¦é«˜ç²¾åº¦ãªå†å‡¦ç†ï¼ˆãƒªãƒ•ã‚¡ã‚¤ãƒ³ï¼‰ã‚’å®Ÿè¡Œã—ã¾ã™ã€‚ä¾‹: --refine page-005.jpg page-012.jpg"
    )
    args = parser.parse_args()

    print(f"ğŸ¤– Ollama ({OLLAMA_MODEL}ãƒ¢ãƒ‡ãƒ«) ã‚’åˆæœŸåŒ–ã—ã¾ã™...")
    try:
        # temperatureã‚’ä½ã‚ã«è¨­å®šã—ã€ã‚ˆã‚Šå¿ å®Ÿãªå‡ºåŠ›ã‚’ä¿ƒã™
        chat = ChatOllama(model=OLLAMA_MODEL, temperature=0.1)
    except Exception as e:
        print(f"âŒ Ollamaãƒ¢ãƒ‡ãƒ«ã®åˆæœŸåŒ–ã«å¤±æ•—ã—ã¾ã—ãŸã€‚è©³ç´°: {e}")
        return

    if args.refine:
        run_refine_process(chat, args.refine)
    else:
        try:
            all_files = sorted([f for f in os.listdir(INPUT_DIR) if f.startswith("page-") and f.lower().endswith(".jpg")])
            run_basic_process(chat, all_files)
        except FileNotFoundError:
            print(f"âŒ ã‚¨ãƒ©ãƒ¼: å…¥åŠ›ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒª '{INPUT_DIR}' ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚")
            return

    print("\n--- ã™ã¹ã¦ã®å‡¦ç†ãŒå®Œäº†ã—ã¾ã—ãŸ ---")


if __name__ == "__main__":
    main()
